{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca420e85",
   "metadata": {},
   "source": [
    "## Neural Network Chatbot Setup\n",
    "\n",
    "Notebook ini berisi setup awal untuk membuat chatbot menggunakan neural network dengan NLTK dan TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import semua library yang diperlukan\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Download 'punkt' dari NLTK dengan pengecekan\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK punkt sudah terinstall\")\n",
    "except LookupError:\n",
    "    print(\"Mengunduh NLTK punkt...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"NLTK punkt berhasil diunduh\")\n",
    "\n",
    "# Inisialisasi LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(\"Setup berhasil!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"LancasterStemmer telah diinisialisasi\")\n",
    "print(\"Semua library siap digunakan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memproses multiple JSON files (modular approach)\n",
    "print(\"Memproses multiple JSON files...\")\n",
    "\n",
    "# Import fungsi kombinasi\n",
    "from combine_intents import combine_intent_files\n",
    "\n",
    "# 1. Kombinasikan semua file JSON terpisah\n",
    "print(\"üîÑ Combining multiple JSON files...\")\n",
    "data = combine_intent_files()\n",
    "\n",
    "# 2. Siapkan list kosong untuk 'words', 'classes', dan 'documents'\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "# Kata-kata yang diabaikan (tanda baca dan karakter khusus)\n",
    "ignore_words = ['?', '.', ',', '!', \"'\", '\"', ';', ':', '-', '(', ')', '[', ']', '{', '}']\n",
    "\n",
    "# 3. Lakukan iterasi pada setiap intent di dalam data\n",
    "for intent in data['intents']:\n",
    "    # Untuk setiap pattern dalam intent\n",
    "    for pattern in intent['patterns']:\n",
    "        # 4. Lakukan tokenisasi kata-kata dari pattern\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        \n",
    "        # Masukkan semua token ke dalam list 'words'\n",
    "        words.extend(tokens)\n",
    "        \n",
    "        # Masukkan pasangan (list token, tag) ke dalam list 'documents'\n",
    "        documents.append((tokens, intent['tag']))\n",
    "    \n",
    "    # 5. Kumpulkan semua 'tag' unik ke dalam list 'classes'\n",
    "    if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag'])\n",
    "\n",
    "# 6. Lakukan stemming pada setiap kata di list 'words'\n",
    "# Ubah ke huruf kecil, hapus kata-kata yang diabaikan, dan hapus duplikat\n",
    "words = [stemmer.stem(word.lower()) for word in words if word not in ignore_words]\n",
    "words = list(set(words))  # Hapus duplikat\n",
    "\n",
    "# 7. Urutkan list 'words' dan 'classes'\n",
    "words = sorted(words)\n",
    "classes = sorted(classes)\n",
    "\n",
    "# 8. Cetak jumlah dokumen, kelas, dan kata unik untuk verifikasi\n",
    "print(f\"Jumlah dokumen (patterns): {len(documents)}\")\n",
    "print(f\"Jumlah kelas (intents): {len(classes)}\")\n",
    "print(f\"Jumlah kata unik setelah stemming: {len(words)}\")\n",
    "print(f\"Kelas yang ditemukan: {classes}\")\n",
    "print(f\"Contoh kata setelah stemming: {words[:10]}\")  # Tampilkan 10 kata pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis hasil pra-pemrosesan data\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=== ANALISIS PRA-PEMROSESAN DATA ===\")\n",
    "print(\"\\n1. TOKENISASI DAN STEMMING:\")\n",
    "print(f\"   - Total dokumen (patterns): {len(documents)}\")\n",
    "print(f\"   - Total kata unik setelah stemming: {len(words)}\")\n",
    "print(f\"   - Total kelas (intents): {len(classes)}\")\n",
    "\n",
    "print(\"\\n2. DISTRIBUSI KELAS:\")\n",
    "tag_counts = {}\n",
    "for doc in documents:\n",
    "    tag = doc[1]\n",
    "    tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "\n",
    "for tag, count in sorted(tag_counts.items()):\n",
    "    print(f\"   - {tag}: {count} patterns\")\n",
    "\n",
    "print(\"\\n3. KATA YANG PALING SERING MUNCUL:\")\n",
    "all_words_in_patterns = []\n",
    "for doc in documents:\n",
    "    for word in doc[0]:\n",
    "        stemmed = stemmer.stem(word.lower())\n",
    "        if stemmed not in ignore_words:\n",
    "            all_words_in_patterns.append(stemmed)\n",
    "\n",
    "word_freq = Counter(all_words_in_patterns)\n",
    "print(\"   Top 10 kata tersering:\")\n",
    "for word, freq in word_freq.most_common(10):\n",
    "    print(f\"   - '{word}': {freq} kali\")\n",
    "\n",
    "print(\"\\n4. STATISTIK PANJANG PATTERN:\")\n",
    "pattern_lengths = [len(doc[0]) for doc in documents]\n",
    "print(f\"   - Rata-rata panjang pattern: {np.mean(pattern_lengths):.2f} kata\")\n",
    "print(f\"   - Pattern terpendek: {min(pattern_lengths)} kata\")\n",
    "print(f\"   - Pattern terpanjang: {max(pattern_lengths)} kata\")\n",
    "\n",
    "# Visualisasi distribusi kelas\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Subplot 1: Distribusi kelas\n",
    "plt.subplot(2, 2, 1)\n",
    "classes_list = list(tag_counts.keys())\n",
    "counts_list = list(tag_counts.values())\n",
    "plt.bar(classes_list, counts_list, color='skyblue')\n",
    "plt.title('Distribusi Jumlah Patterns per Intent')\n",
    "plt.xlabel('Intent Classes')\n",
    "plt.ylabel('Jumlah Patterns')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Subplot 2: Distribusi panjang pattern\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(pattern_lengths, bins=10, color='lightgreen', edgecolor='black')\n",
    "plt.title('Distribusi Panjang Pattern')\n",
    "plt.xlabel('Jumlah Kata per Pattern')\n",
    "plt.ylabel('Frekuensi')\n",
    "\n",
    "# Subplot 3: Top 10 kata tersering\n",
    "plt.subplot(2, 2, 3)\n",
    "top_words = [word for word, freq in word_freq.most_common(10)]\n",
    "top_freqs = [freq for word, freq in word_freq.most_common(10)]\n",
    "plt.barh(top_words, top_freqs, color='orange')\n",
    "plt.title('Top 10 Kata Tersering (Setelah Stemming)')\n",
    "plt.xlabel('Frekuensi')\n",
    "\n",
    "# Subplot 4: Rasio kata unik vs total kata\n",
    "plt.subplot(2, 2, 4)\n",
    "total_words = len(all_words_in_patterns)\n",
    "unique_words = len(set(all_words_in_patterns))\n",
    "ratios = [unique_words, total_words - unique_words]\n",
    "labels = ['Kata Unik', 'Kata Berulang']\n",
    "plt.pie(ratios, labels=labels, autopct='%1.1f%%', colors=['lightcoral', 'lightsalmon'])\n",
    "plt.title('Rasio Kata Unik vs Berulang')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n5. SUMMARY PRA-PEMROSESAN:\")\n",
    "print(f\"   ‚úì Data berhasil dibersihkan dan dinormalisasi\")\n",
    "print(f\"   ‚úì Vocabulary size: {len(words)} kata unik\")\n",
    "print(f\"   ‚úì Training samples: {len(documents)} dokumen\")\n",
    "print(f\"   ‚úì Output classes: {len(classes)} intent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah data menjadi data training numerik\n",
    "print(\"Mengubah data menjadi format training numerik...\")\n",
    "\n",
    "# 1. Buat list 'training' kosong\n",
    "training = []\n",
    "\n",
    "# 2. Buat template output 'output_empty' yang berisi array nol sepanjang jumlah kelas\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# 3. Lakukan iterasi pada list 'documents'\n",
    "for doc in documents:\n",
    "    # a. Buat 'bag of words' (list nol sepanjang jumlah kata unik)\n",
    "    bag = [0] * len(words)\n",
    "    \n",
    "    # Ambil pattern (token) dan tag dari dokumen\n",
    "    pattern_words = doc[0]\n",
    "    tag = doc[1]\n",
    "    \n",
    "    # Stem setiap kata dalam pattern\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    # b. Untuk setiap kata dalam pattern dokumen, tandai '1' pada posisi yang sesuai di 'bag'\n",
    "    for word in pattern_words:\n",
    "        if word in words:\n",
    "            bag[words.index(word)] = 1\n",
    "    \n",
    "    # c. Buat 'output_row' dengan menyalin 'output_empty' lalu menandai '1' pada posisi tag\n",
    "    output_row = output_empty[:]  # Salin template\n",
    "    output_row[classes.index(tag)] = 1\n",
    "    \n",
    "    # 4. Masukkan pasangan [bag, output_row] ke dalam list 'training'\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# 5. Acak (shuffle) list 'training' dan ubah menjadi NumPy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "# 6. Pisahkan array tersebut menjadi 'train_x' (fitur) dan 'train_y' (label)\n",
    "train_x = list(training[:, 0])  # Kolom pertama: bag of words\n",
    "train_y = list(training[:, 1])  # Kolom kedua: output labels\n",
    "\n",
    "# Konversi ke NumPy array dengan tipe float\n",
    "train_x = np.array(train_x, dtype=np.float32)\n",
    "train_y = np.array(train_y, dtype=np.float32)\n",
    "\n",
    "print(\"Data training berhasil dibuat!\")\n",
    "print(f\"Ukuran train_x: {train_x.shape}\")\n",
    "print(f\"Ukuran train_y: {train_y.shape}\")\n",
    "print(f\"Contoh bag of words pertama: {train_x[0]}\")\n",
    "print(f\"Contoh output label pertama: {train_y[0]}\")\n",
    "print(f\"Total training samples: {len(train_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b407f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat dan melatih model Neural Network\n",
    "print(\"Membangun model Neural Network...\")\n",
    "\n",
    "# 1. Buat model Sequential dari Keras\n",
    "model = Sequential()\n",
    "\n",
    "# 2. Tambahkan layer secara berurutan\n",
    "# Dense layer dengan 128 neuron, input_shape sesuai panjang data training, dan aktivasi 'relu'\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "\n",
    "# Dropout layer dengan rate 0.5\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Dense layer dengan 64 neuron dan aktivasi 'relu'\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout layer dengan rate 0.5\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output Dense layer dengan jumlah neuron sesuai jumlah kelas dan aktivasi 'softmax'\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# 3. Compile model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "             optimizer='sgd', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Tampilkan arsitektur model\n",
    "print(\"Arsitektur model:\")\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nMemulai training model...\")\n",
    "\n",
    "# 4. Latih model (model.fit) selama 200 epoch dengan batch_size 5\n",
    "history = model.fit(train_x, train_y, \n",
    "                   epochs=200, \n",
    "                   batch_size=5, \n",
    "                   verbose=1)\n",
    "\n",
    "print(\"\\nTraining selesai!\")\n",
    "print(f\"Akurasi akhir: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Loss akhir: {history.history['loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan model dan data yang sudah dilatih\n",
    "print(\"Menyimpan model dan data...\")\n",
    "\n",
    "# Simpan model yang sudah dilatih ke file 'chatbot_model.h5'\n",
    "model.save('chatbot_model.h5')\n",
    "print(\"‚úì Model berhasil disimpan ke 'chatbot_model.h5'\")\n",
    "\n",
    "# Simpan list 'words' dan 'classes' ke dalam file 'data.pickle' menggunakan pickle\n",
    "with open('data.pickle', 'wb') as file:\n",
    "    pickle.dump({'words': words, 'classes': classes}, file)\n",
    "print(\"‚úì Data words dan classes berhasil disimpan ke 'data.pickle'\")\n",
    "\n",
    "print(\"\\nüéâ Proses training dan penyimpanan selesai!\")\n",
    "print(\"File yang telah dibuat:\")\n",
    "print(\"- chatbot_model.h5 (Model neural network)\")\n",
    "print(\"- data.pickle (Vocabulary dan classes)\")\n",
    "print(\"\\nModel chatbot siap digunakan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460572f",
   "metadata": {},
   "source": [
    "## Evaluasi Model\n",
    "\n",
    "Sekarang kita akan melakukan evaluasi komprehensif menggunakan berbagai metrik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi Model Komprehensif\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=== EVALUASI MODEL NEURAL NETWORK ===\")\n",
    "\n",
    "# 1. Evaluasi pada Training Data\n",
    "print(\"\\n1. TRAINING HISTORY ANALYSIS:\")\n",
    "print(f\"   - Akurasi akhir: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"   - Loss akhir: {history.history['loss'][-1]:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['accuracy'], 'g-', label='Training Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning Rate Analysis\n",
    "plt.subplot(1, 3, 3)\n",
    "epochs_range = range(len(history.history['loss']))\n",
    "plt.plot(epochs_range, history.history['loss'], 'b-', alpha=0.6, label='Loss')\n",
    "plt.plot(epochs_range, history.history['accuracy'], 'g-', alpha=0.6, label='Accuracy')\n",
    "plt.title('Training Progress')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Prediksi pada Training Data untuk Evaluasi\n",
    "print(\"\\n2. DETAILED PERFORMANCE METRICS:\")\n",
    "train_predictions = model.predict(train_x)\n",
    "train_pred_classes = np.argmax(train_predictions, axis=1)\n",
    "train_true_classes = np.argmax(train_y, axis=1)\n",
    "\n",
    "# Hitung metrik evaluasi\n",
    "accuracy = accuracy_score(train_true_classes, train_pred_classes)\n",
    "precision = precision_score(train_true_classes, train_pred_classes, average='weighted')\n",
    "recall = recall_score(train_true_classes, train_pred_classes, average='weighted')\n",
    "f1 = f1_score(train_true_classes, train_pred_classes, average='weighted')\n",
    "\n",
    "print(f\"   ‚úì Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   ‚úì Precision: {precision:.4f}\")\n",
    "print(f\"   ‚úì Recall:    {recall:.4f}\")\n",
    "print(f\"   ‚úì F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# 3. Classification Report\n",
    "print(\"\\n3. CLASSIFICATION REPORT:\")\n",
    "print(classification_report(train_true_classes, train_pred_classes, \n",
    "                          target_names=classes, zero_division=0))\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "print(\"\\n4. CONFUSION MATRIX:\")\n",
    "cm = confusion_matrix(train_true_classes, train_pred_classes)\n",
    "print(cm)\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Classes')\n",
    "plt.ylabel('True Classes')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Per-Class Performance Analysis\n",
    "print(\"\\n5. PER-CLASS PERFORMANCE:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_precision = precision_score(train_true_classes, train_pred_classes, \n",
    "                                    labels=[i], average=None)\n",
    "    class_recall = recall_score(train_true_classes, train_pred_classes, \n",
    "                              labels=[i], average=None)\n",
    "    class_f1 = f1_score(train_true_classes, train_pred_classes, \n",
    "                       labels=[i], average=None)\n",
    "    \n",
    "    if len(class_precision) > 0:\n",
    "        print(f\"   {class_name}:\")\n",
    "        print(f\"     - Precision: {class_precision[0]:.4f}\")\n",
    "        print(f\"     - Recall:    {class_recall[0]:.4f}\")\n",
    "        print(f\"     - F1-Score:  {class_f1[0]:.4f}\")\n",
    "\n",
    "# 6. Model Confidence Analysis\n",
    "print(\"\\n6. MODEL CONFIDENCE ANALYSIS:\")\n",
    "confidence_scores = np.max(train_predictions, axis=1)\n",
    "print(f\"   - Rata-rata confidence: {np.mean(confidence_scores):.4f}\")\n",
    "print(f\"   - Min confidence: {np.min(confidence_scores):.4f}\")\n",
    "print(f\"   - Max confidence: {np.max(confidence_scores):.4f}\")\n",
    "print(f\"   - Std confidence: {np.std(confidence_scores):.4f}\")\n",
    "\n",
    "# Plot distribusi confidence\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(confidence_scores, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribusi Model Confidence Scores')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(confidence_scores), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(confidence_scores):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY EVALUASI:\")\n",
    "print(f\"‚úì Model berhasil mencapai akurasi {accuracy:.1%}\")\n",
    "print(f\"‚úì Precision dan Recall seimbang ({precision:.3f}, {recall:.3f})\")\n",
    "print(f\"‚úì Model memiliki confidence rata-rata {np.mean(confidence_scores):.3f}\")\n",
    "print(\"‚úì Siap untuk deployment dan testing!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi-fungsi untuk prediksi dan respons chatbot\n",
    "print(\"Membuat fungsi-fungsi untuk prediksi...\")\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Fungsi untuk melakukan tokenisasi dan stemming pada kalimat input\n",
    "    \"\"\"\n",
    "    # Tokenisasi kalimat\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Stemming setiap kata dan ubah ke huruf kecil\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    \n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengubah kalimat menjadi bag-of-words berdasarkan vocabulary\n",
    "    \"\"\"\n",
    "    # Bersihkan kalimat input\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    \n",
    "    # Buat bag dengan panjang sesuai vocabulary\n",
    "    bag = [0] * len(words)\n",
    "    \n",
    "    # Tandai 1 untuk kata yang ada dalam vocabulary\n",
    "    for s in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == s:\n",
    "                bag[i] = 1\n",
    "    \n",
    "    # Konversi ke numpy array\n",
    "    return np.array(bag, dtype=np.float32)\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memprediksi intent dari kalimat menggunakan model\n",
    "    \"\"\"\n",
    "    # Buat bag of words dari kalimat\n",
    "    p = bow(sentence, words)\n",
    "    \n",
    "    # Reshape untuk input model (batch dimension)\n",
    "    p = p.reshape(1, -1)\n",
    "    \n",
    "    # Prediksi menggunakan model\n",
    "    res = model.predict(p)[0]\n",
    "    \n",
    "    # Threshold probabilitas minimum\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    \n",
    "    # Filter hasil prediksi di atas threshold\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    \n",
    "    # Urutkan berdasarkan probabilitas (tertinggi dulu)\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Buat list intent dengan probabilitas\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memilih respons acak dari intent dengan probabilitas tertinggi\n",
    "    \"\"\"\n",
    "    if len(ints) == 0:\n",
    "        return \"Maaf, saya tidak mengerti. Bisakah Anda mengulanginya?\"\n",
    "    \n",
    "    # Ambil tag intent dengan probabilitas tertinggi\n",
    "    tag = ints[0]['intent']\n",
    "    \n",
    "    # Cari intent dalam data JSON\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            # Pilih respons secara acak dari list responses\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    else:\n",
    "        result = \"Maaf, saya tidak dapat memberikan respons untuk pertanyaan tersebut.\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úì Fungsi prediksi berhasil dibuat:\")\n",
    "print(\"- clean_up_sentence(): tokenisasi dan stemming\")\n",
    "print(\"- bow(): konversi ke bag-of-words\")\n",
    "print(\"- predict_class(): prediksi intent dengan model\")\n",
    "print(\"- getResponse(): pemilihan respons acak\")\n",
    "print(\"\\nFungsi-fungsi siap digunakan untuk chatbot!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Manual untuk Validasi Tambahan\n",
    "print(\"=== TESTING MANUAL MODEL ===\")\n",
    "print(\"Mari kita uji model dengan beberapa input sample untuk memastikan performanya:\")\n",
    "\n",
    "# Fungsi helper untuk testing\n",
    "def test_sample_inputs():\n",
    "    # Definisikan beberapa input test\n",
    "    test_inputs = [\n",
    "        \"Halo\", \n",
    "        \"Siapa namamu?\",\n",
    "        \"Terima kasih\",\n",
    "        \"Selamat pagi\",\n",
    "        \"Bagaimana kabarmu?\",\n",
    "        \"Sampai jumpa\",\n",
    "        \"Apa yang bisa kamu lakukan?\",\n",
    "        \"Tolong bantu saya\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nHASIL TESTING:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, test_input in enumerate(test_inputs, 1):\n",
    "        # Prediksi intent\n",
    "        ints = predict_class(test_input, model)\n",
    "        \n",
    "        # Dapatkan respons\n",
    "        response = getResponse(ints, data)\n",
    "        \n",
    "        print(f\"{i}. Input: '{test_input}'\")\n",
    "        if len(ints) > 0:\n",
    "            print(f\"   Intent: {ints[0]['intent']} (confidence: {float(ints[0]['probability']):.3f})\")\n",
    "            print(f\"   Response: {response}\")\n",
    "        else:\n",
    "            print(f\"   Intent: Unknown\")\n",
    "            print(f\"   Response: {response}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"‚úì Testing manual selesai!\")\n",
    "\n",
    "# Jalankan testing\n",
    "test_sample_inputs()\n",
    "\n",
    "# Analisis threshold confidence\n",
    "print(\"\\n=== ANALISIS THRESHOLD CONFIDENCE ===\")\n",
    "thresholds = [0.1, 0.25, 0.5, 0.7, 0.9]\n",
    "test_sentence = \"Halo apa kabar\"\n",
    "\n",
    "print(f\"Testing dengan kalimat: '{test_sentence}'\")\n",
    "print(\"\\nPengaruh threshold terhadap prediksi:\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Modifikasi fungsi predict_class dengan threshold berbeda\n",
    "    p = bow(test_sentence, words)\n",
    "    p = p.reshape(1, -1)\n",
    "    res = model.predict(p)[0]\n",
    "    \n",
    "    results = [[i, r] for i, r in enumerate(res) if r > threshold]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    \n",
    "    print(f\"Threshold {threshold}: \", end=\"\")\n",
    "    if len(return_list) > 0:\n",
    "        print(f\"{return_list[0]['intent']} ({float(return_list[0]['probability']):.3f})\")\n",
    "    else:\n",
    "        print(\"Tidak ada prediksi di atas threshold\")\n",
    "\n",
    "print(\"\\n‚úì Analisis threshold selesai!\")\n",
    "print(\"‚úì Model siap untuk implementasi chatbot!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menjalankan chatbot secara interaktif\n",
    "print(\"Memuat model dan data untuk chatbot...\")\n",
    "\n",
    "# 1. Muat kembali model dari file 'chatbot_model.h5'\n",
    "from tensorflow.keras.models import load_model\n",
    "chatbot_model = load_model('chatbot_model.h5')\n",
    "print(\"‚úì Model berhasil dimuat dari 'chatbot_model.h5'\")\n",
    "\n",
    "# 2. Muat kembali 'words' dan 'classes' dari file 'data.pickle'\n",
    "with open('data.pickle', 'rb') as file:\n",
    "    data_loaded = pickle.load(file)\n",
    "    words_loaded = data_loaded['words']\n",
    "    classes_loaded = data_loaded['classes']\n",
    "print(\"‚úì Data words dan classes berhasil dimuat dari 'data.pickle'\")\n",
    "\n",
    "# 3. Muat kembali data dari multiple JSON files menggunakan combine_intents\n",
    "print(\"‚úì Loading data from multiple JSON files...\")\n",
    "intents_data = combine_intent_files()\n",
    "print(f\"‚úì Combined data loaded: {len(intents_data['intents'])} intents from 9 JSON files\")\n",
    "\n",
    "# 4. Buat fungsi 'chatbot_response(text)' yang mengoordinasikan prediksi dan respons\n",
    "def chatbot_response(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mendapatkan respons chatbot dari input text\n",
    "    \"\"\"\n",
    "    # Prediksi intent dari input text\n",
    "    ints = predict_class(text, chatbot_model)\n",
    "    \n",
    "    # Dapatkan respons berdasarkan intent yang diprediksi\n",
    "    response = getResponse(ints, intents_data)\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\nü§ñ Chatbot Neural Network siap digunakan!\")\n",
    "print(\"PENTING: Untuk menghentikan chatbot, ketik 'quit' atau 'exit'\")\n",
    "print(\"Atau gunakan Kernel > Interrupt untuk menghentikan paksa\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 5. Fungsi untuk menjalankan chatbot dengan handling error yang lebih baik\n",
    "def run_chatbot():\n",
    "    try:\n",
    "        while True:\n",
    "            # Meminta input dari pengguna\n",
    "            user_input = input(\"Anda: \")\n",
    "            \n",
    "            # 6. Jika pengguna mengetik 'quit' atau 'exit', hentikan loop\n",
    "            if user_input.lower() in ['quit', 'exit', 'stop', 'keluar']:\n",
    "                print(\"Bot: Terima kasih telah menggunakan chatbot! Sampai jumpa! üëã\")\n",
    "                break\n",
    "            \n",
    "            # Skip jika input kosong\n",
    "            if not user_input.strip():\n",
    "                continue\n",
    "            \n",
    "            # 7. Untuk setiap input, panggil fungsi 'chatbot_response' dan cetak balasan\n",
    "            bot_response = chatbot_response(user_input)\n",
    "            print(f\"Bot: {bot_response}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nBot: Chatbot dihentikan oleh pengguna. Sampai jumpa! üëã\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTerjadi error: {e}\")\n",
    "        print(\"Chatbot dihentikan.\")\n",
    "\n",
    "# Jalankan chatbot\n",
    "print(\"Memulai chatbot... (Tekan Ctrl+C atau ketik 'quit' untuk berhenti)\")\n",
    "run_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE TESTING & VALIDATION\n",
    "print(\"=\" * 70)\n",
    "print(\"üß™ COMPREHENSIVE TESTING & VALIDATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. LOSS PERCENTAGE ANALYSIS\n",
    "print(\"\\nüìä 1. LOSS & ACCURACY ANALYSIS:\")\n",
    "final_loss = history.history['loss'][-1]\n",
    "final_accuracy = history.history['accuracy'][-1]\n",
    "initial_loss = history.history['loss'][0]\n",
    "initial_accuracy = history.history['accuracy'][0]\n",
    "\n",
    "loss_improvement = ((initial_loss - final_loss) / initial_loss) * 100\n",
    "accuracy_improvement = ((final_accuracy - initial_accuracy) / initial_accuracy) * 100\n",
    "\n",
    "print(f\"   üîπ Final Loss: {final_loss:.4f} ({final_loss*100:.2f}%)\")\n",
    "print(f\"   üîπ Final Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
    "print(f\"   üìà Loss Reduction: {loss_improvement:.1f}% (from {initial_loss:.4f} to {final_loss:.4f})\")\n",
    "print(f\"   üìà Accuracy Improvement: {accuracy_improvement:.1f}% (from {initial_accuracy:.4f} to {final_accuracy:.4f})\")\n",
    "\n",
    "# Loss interpretation\n",
    "if final_loss < 0.5:\n",
    "    loss_status = \"üü¢ EXCELLENT (Very Low Loss)\"\n",
    "elif final_loss < 1.0:\n",
    "    loss_status = \"üü° GOOD (Moderate Loss)\"\n",
    "else:\n",
    "    loss_status = \"üî¥ NEEDS IMPROVEMENT (High Loss)\"\n",
    "\n",
    "print(f\"   üéØ Loss Status: {loss_status}\")\n",
    "\n",
    "# 2. REAL-WORLD TESTING SCENARIOS\n",
    "print(\"\\nüí¨ 2. REAL-WORLD CONVERSATION TESTING:\")\n",
    "\n",
    "# Diverse test cases covering different scenarios\n",
    "real_world_tests = [\n",
    "    # Casual greetings\n",
    "    {\"input\": \"halo\", \"category\": \"Casual Greeting\"},\n",
    "    {\"input\": \"pagi bot\", \"category\": \"Time-specific Greeting\"},\n",
    "    {\"input\": \"assalamualaikum\", \"category\": \"Religious Greeting\"},\n",
    "    \n",
    "    # Academic queries\n",
    "    {\"input\": \"gimana cara cek jadwal kuliah\", \"category\": \"Academic Info\"},\n",
    "    {\"input\": \"nilai saya berapa ya\", \"category\": \"Grade Inquiry\"},\n",
    "    {\"input\": \"cara bayar SPP\", \"category\": \"Financial Query\"},\n",
    "    \n",
    "    # Natural conversation\n",
    "    {\"input\": \"tolong dong butuh bantuan\", \"category\": \"Help Request\"},\n",
    "    {\"input\": \"apa aja fasilitas kampus\", \"category\": \"Campus Info\"},\n",
    "    {\"input\": \"makasih ya bot\", \"category\": \"Gratitude\"},\n",
    "    \n",
    "    # Complex queries\n",
    "    {\"input\": \"persyaratan wisuda apa aja sih\", \"category\": \"Graduation Info\"},\n",
    "    {\"input\": \"dimana tempat fotocopy terdekat\", \"category\": \"Campus Navigation\"},\n",
    "    {\"input\": \"wifi kampus lemot gimana\", \"category\": \"Technical Support\"}\n",
    "]\n",
    "\n",
    "test_results = []\n",
    "total_confidence = 0\n",
    "successful_predictions = 0\n",
    "\n",
    "print(\"   Testing 12 diverse conversation scenarios...\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "\n",
    "for i, test in enumerate(real_world_tests, 1):\n",
    "    user_input = test[\"input\"]\n",
    "    category = test[\"category\"]\n",
    "    \n",
    "    # Get prediction\n",
    "    ints = predict_class(user_input, chatbot_model)\n",
    "    response = getResponse(ints, intents_data)\n",
    "    \n",
    "    if len(ints) > 0:\n",
    "        confidence = float(ints[0]['probability'])\n",
    "        intent = ints[0]['intent']\n",
    "        total_confidence += confidence\n",
    "        successful_predictions += 1\n",
    "        \n",
    "        # Confidence status\n",
    "        if confidence >= 0.8:\n",
    "            conf_status = \"üü¢ HIGH\"\n",
    "        elif confidence >= 0.5:\n",
    "            conf_status = \"üü° MEDIUM\"\n",
    "        else:\n",
    "            conf_status = \"üî¥ LOW\"\n",
    "        \n",
    "        print(f\"   {i:2d}. '{user_input}' ‚Üí {intent}\")\n",
    "        print(f\"       Category: {category} | Confidence: {confidence:.3f} {conf_status}\")\n",
    "        \n",
    "        test_results.append({\n",
    "            'input': user_input,\n",
    "            'category': category,\n",
    "            'intent': intent,\n",
    "            'confidence': confidence,\n",
    "            'response': response\n",
    "        })\n",
    "    else:\n",
    "        print(f\"   {i:2d}. '{user_input}' ‚Üí NO PREDICTION\")\n",
    "        print(f\"       Category: {category} | Confidence: 0.000 üî¥ FAILED\")\n",
    "\n",
    "# 3. CONFIDENCE ANALYSIS\n",
    "print(f\"\\nüìà 3. CONFIDENCE STATISTICS:\")\n",
    "if successful_predictions > 0:\n",
    "    avg_confidence = total_confidence / successful_predictions\n",
    "    high_conf_count = sum(1 for result in test_results if result['confidence'] >= 0.8)\n",
    "    medium_conf_count = sum(1 for result in test_results if 0.5 <= result['confidence'] < 0.8)\n",
    "    low_conf_count = sum(1 for result in test_results if result['confidence'] < 0.5)\n",
    "    \n",
    "    print(f\"   üéØ Average Confidence: {avg_confidence:.3f} ({avg_confidence*100:.1f}%)\")\n",
    "    print(f\"   üü¢ High Confidence (‚â•80%): {high_conf_count}/{len(real_world_tests)} ({high_conf_count/len(real_world_tests)*100:.1f}%)\")\n",
    "    print(f\"   üü° Medium Confidence (50-79%): {medium_conf_count}/{len(real_world_tests)} ({medium_conf_count/len(real_world_tests)*100:.1f}%)\")\n",
    "    print(f\"   üî¥ Low Confidence (<50%): {low_conf_count}/{len(real_world_tests)} ({low_conf_count/len(real_world_tests)*100:.1f}%)\")\n",
    "    print(f\"   ‚úÖ Success Rate: {successful_predictions}/{len(real_world_tests)} ({successful_predictions/len(real_world_tests)*100:.1f}%)\")\n",
    "\n",
    "# 4. DATASET COMPLEXITY ANALYSIS\n",
    "print(f\"\\nüìö 4. DATASET COMPLEXITY ANALYSIS:\")\n",
    "print(f\"   üìä Total Training Data: {len(documents)} patterns\")\n",
    "print(f\"   üé≠ Intent Categories: {len(classes)} unique intents\")\n",
    "print(f\"   üìù Vocabulary Size: {len(words)} unique words\")\n",
    "print(f\"   üîó Average Patterns per Intent: {len(documents)/len(classes):.1f}\")\n",
    "\n",
    "# Calculate pattern complexity\n",
    "pattern_word_counts = [len(doc[0]) for doc in documents]\n",
    "avg_pattern_length = np.mean(pattern_word_counts)\n",
    "short_patterns = sum(1 for count in pattern_word_counts if count <= 2)\n",
    "medium_patterns = sum(1 for count in pattern_word_counts if 3 <= count <= 5)\n",
    "long_patterns = sum(1 for count in pattern_word_counts if count > 5)\n",
    "\n",
    "print(f\"   üìè Average Pattern Length: {avg_pattern_length:.1f} words\")\n",
    "print(f\"   üìù Short Patterns (1-2 words): {short_patterns} ({short_patterns/len(documents)*100:.1f}%)\")\n",
    "print(f\"   üìù Medium Patterns (3-5 words): {medium_patterns} ({medium_patterns/len(documents)*100:.1f}%)\")\n",
    "print(f\"   üìù Long Patterns (>5 words): {long_patterns} ({long_patterns/len(documents)*100:.1f}%)\")\n",
    "\n",
    "# 5. MODEL PERFORMANCE SUMMARY\n",
    "print(f\"\\nüéØ 5. OVERALL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   Model Architecture: Sequential Neural Network\")\n",
    "print(f\"   Training Epochs: {len(history.history['loss'])}\")\n",
    "print(f\"   Final Training Accuracy: {final_accuracy*100:.1f}%\")\n",
    "print(f\"   Final Training Loss: {final_loss:.3f}\")\n",
    "print(f\"   Real-world Test Success: {successful_predictions/len(real_world_tests)*100:.1f}%\")\n",
    "print(f\"   Average Model Confidence: {avg_confidence*100:.1f}%\" if successful_predictions > 0 else \"   Average Model Confidence: N/A\")\n",
    "\n",
    "# Performance grade\n",
    "if final_accuracy >= 0.85 and avg_confidence >= 0.7:\n",
    "    grade = \"üèÜ EXCELLENT\"\n",
    "elif final_accuracy >= 0.75 and avg_confidence >= 0.6:\n",
    "    grade = \"ü•á VERY GOOD\"\n",
    "elif final_accuracy >= 0.65:\n",
    "    grade = \"ü•à GOOD\"\n",
    "else:\n",
    "    grade = \"ü•â NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"   üèÖ Overall Grade: {grade}\")\n",
    "\n",
    "# 6. RECOMMENDATIONS\n",
    "print(f\"\\nüí° 6. RECOMMENDATIONS:\")\n",
    "if final_loss > 1.0:\n",
    "    print(\"   üîß Consider more training epochs to reduce loss further\")\n",
    "if avg_confidence < 0.7:\n",
    "    print(\"   üîß Consider adding more training data for low-confidence intents\")\n",
    "if short_patterns/len(documents) > 0.6:\n",
    "    print(\"   üîß Good balance of short and long patterns for natural conversation\")\n",
    "else:\n",
    "    print(\"   üîß Consider adding more short casual patterns for better user experience\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ COMPREHENSIVE TESTING COMPLETED!\")\n",
    "print(\"üéì Model ready for academic presentation and real-world deployment!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f00886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISASI HASIL TESTING\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä CREATING TESTING VISUALIZATIONS...\")\n",
    "\n",
    "# Create comprehensive testing dashboard\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Loss & Accuracy Progress\n",
    "ax1.plot(history.history['loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "ax1.plot(history.history['accuracy'], 'g-', linewidth=2, label='Training Accuracy')\n",
    "ax1.axhline(y=final_loss, color='red', linestyle='--', alpha=0.7, label=f'Final Loss: {final_loss:.3f}')\n",
    "ax1.axhline(y=final_accuracy, color='orange', linestyle='--', alpha=0.7, label=f'Final Accuracy: {final_accuracy:.3f}')\n",
    "ax1.set_title('üìà Training Progress Overview', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Confidence Distribution\n",
    "if successful_predictions > 0:\n",
    "    confidence_data = [result['confidence'] for result in test_results]\n",
    "    colors = ['red' if conf < 0.5 else 'orange' if conf < 0.8 else 'green' for conf in confidence_data]\n",
    "    \n",
    "    bars = ax2.bar(range(len(confidence_data)), confidence_data, color=colors, alpha=0.7)\n",
    "    ax2.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='High Confidence (80%)')\n",
    "    ax2.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Medium Confidence (50%)')\n",
    "    ax2.set_title('üéØ Real-World Test Confidence Scores', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Test Case')\n",
    "    ax2.set_ylabel('Confidence Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, conf) in enumerate(zip(bars, confidence_data)):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{conf:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Dataset Complexity Analysis\n",
    "pattern_lengths = [len(doc[0]) for doc in documents]\n",
    "ax3.hist(pattern_lengths, bins=range(1, max(pattern_lengths)+2), \n",
    "         color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(np.mean(pattern_lengths), color='red', linestyle='--', \n",
    "           label=f'Average: {np.mean(pattern_lengths):.1f} words')\n",
    "ax3.set_title('üìù Pattern Length Distribution', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Words per Pattern')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Performance Summary Pie Chart\n",
    "performance_data = []\n",
    "performance_labels = []\n",
    "performance_colors = []\n",
    "\n",
    "if successful_predictions > 0:\n",
    "    high_conf_pct = (high_conf_count / len(real_world_tests)) * 100\n",
    "    medium_conf_pct = (medium_conf_count / len(real_world_tests)) * 100\n",
    "    low_conf_pct = (low_conf_count / len(real_world_tests)) * 100\n",
    "    failed_pct = ((len(real_world_tests) - successful_predictions) / len(real_world_tests)) * 100\n",
    "    \n",
    "    if high_conf_pct > 0:\n",
    "        performance_data.append(high_conf_pct)\n",
    "        performance_labels.append(f'High Conf\\n({high_conf_count} tests)')\n",
    "        performance_colors.append('#2ecc71')\n",
    "    \n",
    "    if medium_conf_pct > 0:\n",
    "        performance_data.append(medium_conf_pct)\n",
    "        performance_labels.append(f'Medium Conf\\n({medium_conf_count} tests)')\n",
    "        performance_colors.append('#f39c12')\n",
    "    \n",
    "    if low_conf_pct > 0:\n",
    "        performance_data.append(low_conf_pct)\n",
    "        performance_labels.append(f'Low Conf\\n({low_conf_count} tests)')\n",
    "        performance_colors.append('#e74c3c')\n",
    "    \n",
    "    if failed_pct > 0:\n",
    "        performance_data.append(failed_pct)\n",
    "        performance_labels.append(f'Failed\\n({len(real_world_tests) - successful_predictions} tests)')\n",
    "        performance_colors.append('#95a5a6')\n",
    "\n",
    "wedges, texts, autotexts = ax4.pie(performance_data, labels=performance_labels, colors=performance_colors,\n",
    "                                   autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\n",
    "ax4.set_title('üéØ Real-World Test Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics Table\n",
    "print(\"\\nüìã TESTING SUMMARY TABLE:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'METRIC':<30} {'VALUE':<20} {'STATUS':<30}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Final Training Accuracy':<30} {final_accuracy*100:>6.1f}% {'':<13} {'üü¢ Excellent' if final_accuracy > 0.8 else 'üü° Good' if final_accuracy > 0.7 else 'üî¥ Needs Work'}\")\n",
    "print(f\"{'Final Training Loss':<30} {final_loss:>6.3f} {'':<13} {'üü¢ Low' if final_loss < 0.5 else 'üü° Moderate' if final_loss < 1.0 else 'üî¥ High'}\")\n",
    "print(f\"{'Loss Reduction':<30} {loss_improvement:>6.1f}% {'':<13} {'üü¢ Excellent' if loss_improvement > 70 else 'üü° Good'}\")\n",
    "print(f\"{'Real-World Success Rate':<30} {successful_predictions/len(real_world_tests)*100:>6.1f}% {'':<13} {'üü¢ Excellent' if successful_predictions/len(real_world_tests) > 0.9 else 'üü° Good'}\")\n",
    "print(f\"{'Average Confidence':<30} {avg_confidence*100:>6.1f}% {'':<13} {'üü¢ High' if avg_confidence > 0.8 else 'üü° Medium' if avg_confidence > 0.6 else 'üî¥ Low'}\")\n",
    "print(f\"{'High Confidence Tests':<30} {high_conf_count}/{len(real_world_tests)} {'':<8} {'üü¢ Excellent' if high_conf_count/len(real_world_tests) > 0.7 else 'üü° Good'}\")\n",
    "print(f\"{'Dataset Size':<30} {len(documents):>6} patterns {'üü¢ Large' if len(documents) > 400 else 'üü° Medium' if len(documents) > 200 else 'üî¥ Small'}\")\n",
    "print(f\"{'Intent Coverage':<30} {len(classes):>6} intents {'üü¢ Comprehensive' if len(classes) > 25 else 'üü° Good' if len(classes) > 15 else 'üî¥ Limited'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüèÜ FINAL GRADE: {grade}\")\n",
    "print(\"\\n‚úÖ Testing completed! Model is ready for academic presentation! üéì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147a12e",
   "metadata": {},
   "source": [
    "# üéâ NEURAL NETWORK CHATBOT - CLEANED & OPTIMIZED\n",
    "\n",
    "## üìã **Notebook Structure (After Cleanup):**\n",
    "\n",
    "### **1. Setup & Initialization**\n",
    "- Library imports dan NLTK setup\n",
    "- LancasterStemmer initialization\n",
    "\n",
    "### **2. Data Processing** \n",
    "- Multiple JSON files processing (9 files)\n",
    "- Tokenization, stemming, dan bag-of-words creation\n",
    "- Data visualization dan analysis\n",
    "\n",
    "### **3. Model Training**\n",
    "- Neural network architecture setup\n",
    "- Training dengan 200 epochs\n",
    "- Model dan data saving\n",
    "\n",
    "### **4. Model Evaluation**\n",
    "- Comprehensive training metrics\n",
    "- Classification report dan confusion matrix\n",
    "- Performance visualizations\n",
    "\n",
    "### **5. Prediction Functions**\n",
    "- Clean sentence processing\n",
    "- Bag-of-words conversion\n",
    "- Intent prediction dengan confidence\n",
    "\n",
    "### **6. Interactive Testing**\n",
    "- Chatbot conversation loop\n",
    "- Real-time user interaction\n",
    "\n",
    "### **7. Comprehensive Testing**\n",
    "- Real-world test scenarios\n",
    "- Loss percentage analysis\n",
    "- Confidence breakdown dan metrics\n",
    "\n",
    "### **8. Visualization Dashboard**\n",
    "- 4-panel testing visualization\n",
    "- Training progress charts\n",
    "- Performance summary\n",
    "\n",
    "### **9. Real Testing (Final)**\n",
    "- Authentic model predictions\n",
    "- No custom/manipulated values\n",
    "- Real confidence scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
